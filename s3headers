#! /usr/bin/env python

import ConfigParser, argparse, os

from boto.s3.connection import S3Connection
from datetime import datetime, timedelta


parser = argparse.ArgumentParser(description='Sets 1 year caching headers on S3 for the specified files.')
parser.add_argument('files', metavar='FILE', nargs='+', help='a file to print out the headers for' )

args = parser.parse_args()
file_list = args.files

#print file_list

if os.path.exists(os.path.expanduser('~/.aws')):
	aws_credentials_path = os.path.expanduser('~/.aws')
	config  =   ConfigParser.SafeConfigParser()
	config.readfp(open(aws_credentials_path))

	if config.has_section('s3'):

		if config.has_option('s3', 'access_id'):

			S3_ACCESS_ID = config.get('s3', 'access_id')

			#print 'grabbed s3.access_id'

		else:

			print '.aws config file missing acess_id'

			sys.exit()

		if config.has_option('s3', 'secret_key'):

			S3_SECRET_KEY = config.get('s3', 'secret_key')

			#print 'grabbed s3.secret_key'

		else:

			print '.aws config file missing secret_key'

			sys.exit()

		if config.has_option('s3', 'bucket'):

			S3_BUCKET = config.get('s3', 'bucket')

		else:

			print '.aws config file missing bucket'
			sys.exit()

	else:

		print 'No s3 section in .aws config file'

		sys.exit()
else:

	print 'No .aws config file in user directory'

	sys.exit()

conn = S3Connection( S3_ACCESS_ID, S3_SECRET_KEY )
bucket = conn.get_bucket( S3_BUCKET )

for key in file_list:

	remote_file = bucket.get_key( key )
	if remote_file:
		expire_date = datetime.utcnow() + timedelta(days=365)
		expire_date = expire_date.strftime("%a, %d %b %Y %H:%M:%S GMT")
		c_type = remote_file.content_type
		key_metadata = {'Content-Type': c_type, 'Expires': expire_date, 'Cache-Control': 'public, max-age=31536000' }
		remote_file.copy( S3_BUCKET, key, metadata=key_metadata, preserve_acl=True)
	
	else: 
		print 'This file is not on S3: %s'  % key
